# Deep Learning Model Construction & Parameter Management

## Building Deep Learning Models

Deep learning models consist of **layers** that transform input data to learn patterns and make predictions. In PyTorch, we construct models using `nn.Linear` for single layers, `nn.Sequential` for simple stackable models, and `nn.Module` for full model architectures.

## Understanding `nn.Linear`, `nn.Sequential`, and `nn.Module`

### `nn.Linear`: Fully Connected Layers

`nn.Linear(in_features, out_features, bias=True)` defines a **fully connected (dense) layer**, where each input connects to every output through a weight matrix. It is the fundamental building block of most neural networks.

#### Example: Using `nn.Linear`
```python
import torch
from torch import nn

linear_layer = nn.Linear(5, 3)  # 5 input features, 3 output features
x = torch.randn(2, 5)  # Batch of 2 samples, each with 5 features
output = linear_layer(x)

print("Weight matrix:", linear_layer.weight)
print("Bias:", linear_layer.bias)
print("Output:", output)
```

### `nn.Sequential`: Simple Model Stacking

`nn.Sequential` is a **convenience wrapper** that stacks multiple layers together, applying them in order. It is useful for defining feedforward architectures where the data flows sequentially through layers.

#### Example: Using `nn.Sequential`
```python
model = nn.Sequential(
    nn.Linear(5, 3),
    nn.ReLU(),
    nn.Linear(3, 1)
)
```

ðŸ“Œ `nn.Sequential` simplifies model definitions but lacks flexibility for more complex architectures requiring branching or parameter sharing.

### `nn.Module`: The Base Class for Models

`nn.Module` is the foundation for all neural networks in PyTorch. It allows users to define **custom models** that include multiple layers, parameter management, and a `forward()` method to specify the computation.

#### Example: Creating a Model with `nn.Module`
```python
class SimpleModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.fc1 = nn.Linear(5, 3)
        self.fc2 = nn.Linear(3, 1)

    def forward(self, x):
        x = torch.relu(self.fc1(x))
        return self.fc2(x)

model = SimpleModel()
```

[**Demo Code in Colab Notebook**](https://colab.research.google.com/drive/1QkS31O-W8OEaRZoGWz3UvCbAOOJtyeir?usp=sharing)

### Relationship Between `nn.Linear`, `nn.Sequential`, and `nn.Module`

1. **`nn.Linear` provides the fundamental building blocks** (fully connected layers with weights and biases). It is used inside models.
2. **`nn.Sequential` is a simple way to stack multiple `nn.Linear` layers** in a forward sequence without writing a custom forward function.
3. **`nn.Module` is the most flexible option, allowing complex architectures** with multiple layers, loops, and conditional execution.

| Feature | `nn.Linear` | `nn.Sequential` | `nn.Module` |
|---------|------------|----------------|-------------|
| **Definition** | A single **fully connected (dense) layer**. | A container for stacking layers sequentially. | A base class for **building models**. |
| **Usage** | Used **inside** models or as a simple layer. | Used for **simple models** where layers follow sequentially. | Used for **full models**, including complex architectures. |
| **Flexibility** | Only a **single transformation**. | Limited to simple forward layer stacks. | Can contain **multiple layers**, custom logic, and different paths. |
| **Parameter Access** | Has `.weight` and `.bias`. | Automatically applies layers in order. | Stores all parameters in one place. |

## Accessing and Managing Model Parameters

Deep learning models store **trainable parameters** (weights & biases) inside `nn.Module`. 

### Accessing Parameters
```python
for name, param in model.named_parameters():
    print(f"Parameter: {name}, Shape: {param.shape}")
```

## Parameter Initialization

Proper initialization helps models train efficiently by preventing issues like vanishing or exploding gradients.

### Default vs. Custom Initialization
```python
# Default initialization
linear = nn.Linear(5, 3)
print(linear.weight)

# Custom Initialization

def init_weights(m):
    if isinstance(m, nn.Linear):
        nn.init.xavier_uniform_(m.weight)
        nn.init.zeros_(m.bias)

model.apply(init_weights)
```

## Lazy Initialization: Delaying Parameter Creation

Lazy initialization defers parameter creation until the first forward pass, allowing models to adapt dynamically.

### Example: Lazy Initialization in PyTorch
```python
class LazyModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.fc = None  # Placeholder

    def forward(self, x):
        if self.fc is None:
            self.fc = nn.Linear(x.shape[1], 3)
        return self.fc(x)

model = LazyModel()
x = torch.randn(4, 5)
print(model(x).shape)  # Triggers initialization
```

## Custom Layers in PyTorch

We can extend `nn.Module` to build custom layers that modify computations.

### Example: Custom Scaling Layer
```python
class ScalingLayer(nn.Module):
    def __init__(self, scale_factor=1.0):
        super().__init__()
        self.scale = nn.Parameter(torch.tensor(scale_factor))

    def forward(self, x):
        return self.scale * x
```

## Saving and Loading Models in PyTorch

### Saving Model Weights
```python
torch.save(model.state_dict(), "model.pth")
```

### Loading Model Weights
```python
new_model = LazyModel()  # Must define the model architecture first
new_model.load_state_dict(torch.load("model.pth"))
new_model.eval()
```

Calling `eval()` ensures correct behavior for dropout and batch normalization layers.

## Key Takeaways

- **`nn.Linear` provides individual layers, `nn.Sequential` stacks them, and `nn.Module` builds full models.**
- **Model parameters (weights & biases) are stored in `model.parameters()`.**
- **Lazy initialization allows dynamic parameter creation.**
- **Custom layers provide flexibility beyond built-in PyTorch layers.**
- **Saving/loading models lets us reuse trained networks efficiently.**

