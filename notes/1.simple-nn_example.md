# Simple Neural Network Example

## Problem
Predict if my kids will annoy you based on two features:

- **Volume of Noise** (e.g., how loud they are)
- **Frequency of Interruptions** (e.g., how often they disrupt my activities)

## Neural Network Structure

### Inputs
- **Volume of Noise** (on a scale of 1-10)
- **Frequency of Interruptions** (number per hour)

### Weights
- **Weight 1**: 0.7 (importance of noise volume)
- **Weight 2**: 0.5 (importance of interruption frequency)

Each weight represents the strength of a connection between individual neurons and can take on any value (positive or negative), meaning there is no requirement for them to sum to a specific value like 1.

### Bias
- **Bias**: -5 (my baseline tolerance level for annoyance)

### Output Function
The neuron calculates a weighted sum:

`Weighted Sum = (Weight 1 × Input 1) + (Weight 2 × Input 2) + Bias`

The output is passed through a simple activation function:
- If Weighted Sum >= 0, I am annoyed (output = 1).
- If Weighted Sum < 0, I am not annoyed (output = 0).

## Example Calculation

### Case 1: Annoyed
**Input Values:**
- Volume of Noise = 9 (on a scale of 1-10)
- Frequency of Interruptions = 5 (5 interruptions per hour)

**Weighted Sum Calculation:**

`Weighted Sum = (0.3 × 9) + (0.8 × 5) - 5 = 1.7`

**Activation Function:**

Since Weighted Sum = 1.7 is greater than or equal to 0, the output is 1 (I am annoyed).

### Case 2: Not Annoyed
**Input Values:**
- Volume of Noise = 3 (on a scale of 1-10)
- Frequency of Interruptions = 2 (2 interruptions per hour)

**Weighted Sum Calculation:**
`Weighted Sum = (0.3 × 5) + (0.8 × 3) - 5 = -1.1`

**Activation Function:**

Since Weighted Sum = -1.1 is less than 0, the output is 0 (I am not annoyed).

## Backpropagation Step

Backpropagation is used to adjust the weights and bias to minimize the error in the neural network's predictions. Here is a simplified example of how backpropagation might work for this neural network.

### Error Calculation
First, we need to calculate the error for each example. Let's assume the desired output for Case 1 is 1 (Annoyed) and for Case 2 is 0 (Not Annoyed).

For Case 1:

`Error = Desired Output - Actual Output Error = 1 - 1 = 0`

For Case 2:

`Error = Desired Output - Actual Output Error = 0 - 0 = 0`

### Gradient Calculation
Next, we calculate the gradient of the error with respect to each weight and bias. This involves calculating the partial derivatives of the error with respect to each weight and bias. The gradient indicates how much the error will change with a small change in the weight or bias.

For simplicity, let's assume a learning rate (η) of 0.1. The learning rate determines the size of the steps we take to reach the minimum error. A smaller learning rate means smaller steps, which can lead to more precise adjustments but may take longer to converge. A larger learning rate means larger steps, which can speed up convergence but may overshoot the minimum error.

### Weight Updates
Using the gradients, we update the weights and bias. The formulas for updating the weights and bias are as follows:

For Weight 1:
`Weight 1 = Weight 1 - η * (Error * Input 1)`

Similarly, we subtract the product of the learning rate, the error, and the input value from the current weight.

For Weight 2:
`Weight 2 = Weight 2 - η * (Error * Input 2)`

For Bias:
`Bias = Bias - η * Error`

We subtract the product of the learning rate and the error from the current bias.

Since the error for both cases is 0, the weights and bias remain unchanged in this example. However, in a real scenario with non-zero errors, the weights and bias would be adjusted to reduce the error in future predictions.