# LSTMs and GRUs

## Ref: 
- [LSTM Tutorial](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)
- [D2l Chapters 10.1 and 10.2](https://d2l.ai/chapter_recurrent-modern/lstm.html)
  
### Why Do We Need LSTMs and GRUs?

**RNNs:**
- RNNs (Recurrent Neural Networks) process sequential data by maintaining hidden states across time steps.

![Vanilla RNN Architecture](images/rnn.png)

- **Problem:** They struggle with long-term dependencies due to *vanishing* and *exploding* gradients during backpropagation.

**Solution:**
- LSTMs (Long Short-Term Memory) and GRUs (Gated Recurrent Units) help to:
  - Retain relevant information for longer sequences.
  - Control what information to keep or discard.

## 2. Long Short-Term Memory (LSTM) Networks

### Key Idea
LSTMs use memory cells and three *gates* to control the flow of information:

### LSTM Structure
1. **Forget Gate:** Decides what information to discard.
   
   <img src="https://latex.codecogs.com/png.latex?\mathbf{f}_t%20=%20\sigma(\mathbf{W}_f%20[\mathbf{h}_{t-1},%20\mathbf{x}_t]%20+%20\mathbf{b}_f)" />
   
   - **f<sub>t</sub>**: Forget gate output (values between 0 and 1).
   - **σ**: Sigmoid activation function.
   - **W<sub>f</sub>**: Weight matrix for the forget gate.
   - **h<sub>t-1</sub>**: Hidden state from the previous time step.
   - **x<sub>t</sub>**: Input at the current time step.
   - **b<sub>f</sub>**: Bias vector for the forget gate.

2. **Input Gate:** Determines what new information to store.

   <img src="https://latex.codecogs.com/png.latex?\mathbf{i}_t%20=%20\sigma(\mathbf{W}_i%20[\mathbf{h}_{t-1},%20\mathbf{x}_t]%20+%20\mathbf{b}_i)" />
   
   - **i<sub>t</sub>**: Input gate output (values between 0 and 1).
   - **W<sub>i</sub>**: Weight matrix for the input gate.
   - **b<sub>i</sub>**: Bias vector for the input gate.

3. **Cell State Update:** Updates the memory cell.

   <img src="https://latex.codecogs.com/png.latex?\tilde{\mathbf{C}}_t%20=%20\tanh(\mathbf{W}_c%20[\mathbf{h}_{t-1},%20\mathbf{x}_t]%20+%20\mathbf{b}_c)" />
   
   - **C̃<sub>t</sub>**: Candidate cell state (values between -1 and 1).
   - **tanh**: Hyperbolic tangent activation function.
   - **W<sub>c</sub>**: Weight matrix for the candidate cell state.
   - **b<sub>c</sub>**: Bias vector for the candidate cell state.

   <img src="https://latex.codecogs.com/png.latex?\mathbf{C}_t%20=%20\mathbf{f}_t%20\odot%20\mathbf{C}_{t-1}%20+%20\mathbf{i}_t%20\odot%20\tilde{\mathbf{C}}_t" />
   
   - **C<sub>t</sub>**: Updated cell state.
   - **C<sub>t-1</sub>**: Cell state from the previous time step.
   - **⊙**: Element-wise multiplication.

4. **Output Gate:** Controls what information becomes the hidden state.

   <img src="https://latex.codecogs.com/png.latex?\mathbf{o}_t%20=%20\sigma(\mathbf{W}_o%20[\mathbf{h}_{t-1},%20\mathbf{x}_t]%20+%20\mathbf{b}_o)" />
   
   - **o<sub>t</sub>**: Output gate output (values between 0 and 1).
   - **W<sub>o</sub>**: Weight matrix for the output gate.
   - **b<sub>o</sub>**: Bias vector for the output gate.

   <img src="https://latex.codecogs.com/png.latex?\mathbf{h}_t%20=%20\mathbf{o}_t%20\odot%20\tanh(\mathbf{C}_t)" />
   
   - **h<sub>t</sub>**: Hidden state at the current time step.

![LSTM Architecture](images/lstm.png)

### Why Use LSTMs?
- They maintain long-term dependencies.
- Mitigate the vanishing gradient problem.

### Example Code (LSTM)
```python
import torch
from torch import nn

lstm = nn.LSTM(input_size=10, hidden_size=20, num_layers=1)
x = torch.randn(5, 3, 10)  # (sequence_length, batch_size, input_size)
out, (h, c) = lstm(x)
print(out.shape, h.shape, c.shape)
```

## 3. Gated Recurrent Units (GRUs)

### Key Idea
GRUs simplify the LSTM by using only two gates:

### GRU Structure
1. **Update Gate:** Controls how much of the previous information is carried forward.

   <img src="https://latex.codecogs.com/png.latex?\mathbf{z}_t%20=%20\sigma(\mathbf{W}_z%20[\mathbf{h}_{t-1},%20\mathbf{x}_t]%20+%20\mathbf{b}_z)" />
   
   - **z<sub>t</sub>**: Update gate output (values between 0 and 1).
   - **W<sub>z</sub>**: Weight matrix for the update gate.
   - **b<sub>z</sub>**: Bias vector for the update gate.

2. **Reset Gate:** Controls how much of the past information to forget.

   <img src="https://latex.codecogs.com/png.latex?\mathbf{r}_t%20=%20\sigma(\mathbf{W}_r%20[\mathbf{h}_{t-1},%20\mathbf{x}_t]%20+%20\mathbf{b}_r)" />
   
   - **r<sub>t</sub>**: Reset gate output (values between 0 and 1).
   - **W<sub>r</sub>**: Weight matrix for the reset gate.
   - **b<sub>r</sub>**: Bias vector for the reset gate.

3. **Candidate Activation:** Computes a candidate hidden state.

   <img src="https://latex.codecogs.com/png.latex?\tilde{\mathbf{h}}_t%20=%20\tanh(\mathbf{W}_h%20[\mathbf{r}_t%20\odot%20\mathbf{h}_{t-1},%20\mathbf{x}_t]%20+%20\mathbf{b}_h)" />
   
   - **h̃<sub>t</sub>**: Candidate hidden state (values between -1 and 1).
   - **W<sub>h</sub>**: Weight matrix for the candidate hidden state.
   - **b<sub>h</sub>**: Bias vector for the candidate hidden state.

4. **Final Hidden State:** Blends old and new information.

   <img src="https://latex.codecogs.com/png.latex?\mathbf{h}_t%20=%20\mathbf{z}_t%20\odot%20\mathbf{h}_{t-1}%20+%20(1%20-%20\mathbf{z}_t)%20\odot%20\tilde{\mathbf{h}}_t" />
   
   - **h<sub>t</sub>**: Hidden state at the current time step.

![GRU Architecture](images/lstm.png)

### Why Use GRUs?
- Simpler and faster than LSTMs (fewer parameters).
- Suitable for applications where training time is critical.

### Example Code (GRU)
```python
import torch
from torch import nn

gru = nn.GRU(input_size=10, hidden_size=20, num_layers=1)
x = torch.randn(5, 3, 10)  # (sequence_length, batch_size, input_size)
out, h = gru(x)
print(out.shape, h.shape)
```

## 4. Comparing LSTMs and GRUs

| Feature            | LSTM                         | GRU                    |
|--------------------|------------------------------|-------------------------|
| Number of Gates    | 3 (forget, input, output)    | 2 (update, reset)       |
| Complexity         | Higher (more parameters)     | Lower (faster training) |
| Performance        | Better for complex tasks     | Faster for simpler tasks|
| Use Case           | Long sequences, complex data | Faster, simpler models  |

# **Bidirectional Recurrent Neural Networks (Bi-RNNs)**

A **Bidirectional RNN (Bi-RNN)** is a type of recurrent neural network that processes sequences in **two directions**:

- **Forward**: Left to right (past to future)
- **Backward**: Right to left (future to past)

By combining information from *both directions*, Bi-RNNs capture *full context* in a sequence, improving performance on tasks where future context matters.

## **Why Do We Need Bi-RNNs?**
A **standard RNN** only processes data in one direction (usually left to right). This means it cannot capture information from future time steps.

## **How Bi-RNN Works**
1. **Forward Pass:** Process the input sequence from **left to right**.
2. **Backward Pass:** Process the input sequence from **right to left**.
3. **Concatenate Outputs:** Combine hidden states from both directions.
4. **Final Prediction:** Use the combined representation for classification or other tasks.

### Diagram:
```
Forward:    x1 → h1 → h2 → h3 → h4
Backward:   x4 → h4 → h3 → h2 → h1
```

At each time step, the model has **two hidden states** (from the forward and backward passes).

## **Advantages of Bi-RNNs**
- **Better Context Understanding:** Captures both past and future information.
- **Improved Accuracy:** More accurate for tasks like sentiment analysis and machine translation.

### Real-World Applications:
- **Sentiment Analysis:** Understanding nuances in text.
- **Speech Recognition:** Capturing context for ambiguous sounds.
- **Machine Translation:** Using future words to improve translations.

## **Limitations of Bi-RNNs**
- **Higher Computational Cost:** Requires two passes through the data.
- **Increased Memory Usage:** Stores forward and backward states.
- **Slower Inference:** Takes longer to process input.


