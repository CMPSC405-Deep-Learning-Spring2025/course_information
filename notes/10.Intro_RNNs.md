# Sequence Models and Language Models

## Reference: 
- [Sequence Colab Notebook](https://colab.research.google.com/drive/1kagKKCIp0IifZw1UCaMcWF0j8s3waVP5?usp=sharing)
- [SImple RNN Notebook](https://colab.research.google.com/drive/1CXyozxh25-LYTZQ-gXP2bd0LuYLmFdte?usp=sharing)


## 1. Overview
Sequential data, including text, speech, and even DNA, requires models that capture context over time or position. Natural Language Processing (NLP) is a key example where sequence models track linguistic patterns and dependencies within text. Below are some common NLP tasks illustrating the need for sequence-aware approaches:

- Sentiment Analysis: Determining whether a piece of text is positive, negative, or neutral.
  - Example: Analyzing customer reviews of a product to understand overall satisfaction.

- Text Classification: Assigning predefined labels to text.
  - Example: Categorizing emails as "spam" or "not spam."

- Named Entity Recognition (NER): Identifying and classifying proper names in text.
  - Example: Recognizing "New York" as a location and "Elon Musk" as a person in a news article.

- Machine Translation: Translating text from one language to another.
  - Example: Translating a sentence from English to French.

- Speech Recognition: Converting spoken language into written text.
  - Example: Dictating a message to a virtual assistant like Siri or Alexa.

- Text Summarization: Creating a short summary of a longer piece of text.
  - Example: Summarizing an article or research paper into a few sentences.

- Question Answering: Extracting answers to questions from a body of text.
  - Example: Answering "Who won the 2020 U.S. presidential election?" using news articles.

- Part-of-Speech Tagging: Identifying the grammatical parts of a sentence (e.g., nouns, verbs, adjectives).
  - Example: "She quickly ran to the store" → "She (pronoun), quickly (adverb), ran (verb), to (preposition), the (article), store (noun)."

- Coreference Resolution: Determining which words or phrases refer to the same entity.
  - Example: In the sentence "John went to the store. He bought milk." – Resolving that "He" refers to "John."

- Language Generation: Creating coherent and contextually appropriate text.
  - Example: GPT-based models generating conversational responses based on user input.

## 2. Understanding Sequential Data
- **Sequence**: An ordered list of elements (e.g., words in a sentence).
- **Temporal Dependencies**: Earlier elements affect later ones (e.g., meaning of a sentence depends on word order).
- **Examples**:
  - Sentences in natural language processing (NLP)
  - Stock price predictions
  - DNA sequences in bioinformatics

Text must be converted into numbers before models can use it.
- Step 1: Tokenization
Tokenization splits text into words or characters.
[Regular Expression Tester](https://regexr.com/)

- Step 2: Conversion of tokens to numbers
### Word Embeddings and One-Hot Encoding
- **One-hot encoding**: Represent words as vectors with a single 1 (e.g., "cat" = [0,1,0,...,0]).
- **Word embeddings**: Learn dense vector representations (e.g., Word2Vec, GloVe) where similar words have similar embeddings.

## 3. Perplexity and Sequence Partitioning

### 3.1 Perplexity
$$
\text{Perplexity} = \exp\!\Bigl(- \frac{1}{N} \sum_{i=1}^{N} \log P(w_i \mid w_1, \ldots, w_{i-1}) \Bigr)
$$
- **Interpretation**:
  - Lower perplexity = better model.
  - Perplexity of a random model is high, while a perfect model has perplexity close to 1.

### 3.2 Partitioning Sequences
- **Goal**: Prepare text for training by dividing it into manageable chunks.
- **Common Methods**:
  - **Fixed-length partitioning**: Split text into fixed-size sequences.
  - **Overlapping windows**: Ensure continuity across partitions.
  - **Batch processing**: Improves training efficiency.

## 4. N-Gram Models
- **Definition**: An n-gram is a sequence of **n** consecutive words.
- **Example (bigram model, n=2)**:
  - "The cat sits" → P("sits" | "The cat")
- **Limitations**:
  - Fixed window size (cannot capture long-term dependencies).
  - Data sparsity (rare sequences are hard to model).

## 5. Language Modeling
- **Goal**: Predict the next word given a sequence.
- **Markov Assumption**: Probability of a word depends only on the previous **n-1** words.
- **Mathematical Formulation**:  
  Given sequence w1, w2, ..., wT, model: $P(w_T \mid w_1, w_2, \ldots, w_{T-1})$
- **Challenges**:
  - Longer contexts require more data.
  - Computational cost increases with larger n.

### 5.1 Types of Language Models
A language model (LM) assigns probabilities to sequences of words:
$$P(w_{T} \mid w_{1}, w_{2}, \ldots, w_{{T-1}})$$

**Unigram Model (Ignores Order)**
$$P(w_{T}) = \frac{\mathrm{count}(w_{T})}{\mathrm{totalWords}}$$

Problem: It doesn't consider context.

**Bigram Model (Considers One Previous Word)**
$$P(w_{T} \mid w_{{T-1}}) = \frac{\mathrm{count}(w_{{T-1}}, w_{T})}{\mathrm{count}(w_{{T-1}})}$$

**N-gram Model (Considers N−1 Previous Words)**
$$P(w_{T} \mid w_{{T-n+1}}, \ldots, w_{{T-1}})$$

# Recurrent Neural Networks (RNNs) and Backpropagation Through Time (BPTT)

## 1. Introduction to RNNs
Recurrent Neural Networks (RNNs) process sequences step-by-step while maintaining a **hidden state** that captures information from previous inputs. This hidden state acts as memory, enabling the model to learn sequential dependencies. 

- **Why RNNs?**
  - Traditional models (n-grams, bag-of-words) fail to capture long-term dependencies.
  - RNNs maintain a **hidden state** to store past information.

## **Hidden States in RNNs**  

#### **Hidden State Update Equation**  
At each time step $t$, the hidden state $\mathbf{h}_t$ is updated using:

[![Hidden State Update Equation](https://latex.codecogs.com/png.latex?%5Cmathbf%7Bh%7D_t%20%3D%20%5Cphi%28%5Cmathbf%7BW_h%7D%20%5Cmathbf%7Bh%7D_%7Bt-1%7D%20%2B%20%5Cmathbf%7BW_x%7D%20%5Cmathbf%7Bx%7D_t%20%2B%20%5Cmathbf%7Bb%7D%29)](https://latex.codecogs.com/eqneditor/editor.php)

where:  
- $\mathbf{h}_t$ = hidden state at time step $t$  
- $\mathbf{h}_{t-1}$ = hidden state from the previous time step  
- $\mathbf{x}_t$ = input at time step $t$  
- $\mathbf{W_h}$ = weight matrix for the hidden state  
- $\mathbf{W_x}$ = weight matrix for the input  
- $\mathbf{b}$ = bias vector  
- $\phi$ = activation function (typically **tanh** or **ReLU**)  

This equation ensures that past information stored in $\mathbf{h}_{t-1}$ is combined with new input $\mathbf{x}_t$, allowing the network to maintain a memory of past inputs.  

### **Gradient Clipping**
When training recurrent neural networks, gradients can sometimes explode, resulting in very large updates to the model parameters. To address this, we can use **gradient clipping**, which ensures that the gradients do not exceed a certain threshold.

#### **How Gradient Clipping Works**
1. Compute the norm of the gradients across all parameters.
2. If the norm exceeds a predefined threshold, scale the gradients down so that their norm equals the threshold.
3. Otherwise, leave the gradients unchanged.

#### **Mathematical Formulation**
Let $\mathbf{g}$ represent the gradient vector of all model parameters, and let $\theta$ be the clipping threshold. Gradient clipping modifies $\mathbf{g}$ as follows:

$$
\mathbf{g} \leftarrow \mathbf{g} \cdot \min\left(1, \frac{\theta}{\|\mathbf{g}\|}\right)
$$

where $\|\mathbf{g}\|$ is the norm of the gradient vector.

#### **Why Gradient Clipping is Useful**
- Prevents exploding gradients during training.
- Stabilizes the training process, especially for deep or long-sequence models.

#### **Implementation**
Gradient clipping is often implemented in deep learning frameworks as part of the optimization step. For example:
- In PyTorch: `torch.nn.utils.clip_grad_norm_`
- In TensorFlow: `tf.clip_by_norm`

### **Output Computation**  
The hidden state is then used to compute the output $\mathbf{o}_t$ at each time step:  

[![Output Computation Equation](https://latex.codecogs.com/png.latex?%5Cmathbf%7Bo%7D_t%20%3D%20%5Cmathbf%7BW_o%7D%20%5Cmathbf%7Bh%7D_t%20%2B%20%5Cmathbf%7Bb_o%7D)](https://latex.codecogs.com/eqneditor/editor.php)

where:  
- $\mathbf{W_o}$ = weight matrix for mapping the hidden state to the output  
- $\mathbf{b}_o$ = bias term for the output  
- $\mathbf{o}_t$ = output at time $t$  

### **Key Takeaways**  
- The recurrence in $\mathbf{h}_t$ allows the model to propagate information over time.  
- The activation function $\phi$ (e.g., tanh) introduces non-linearity, preventing simple linear dependence on previous states.  
- The weight matrices $\mathbf{W_h}$ and $\mathbf{W_x}$ are **learned during training**, allowing the model to extract meaningful sequential patterns.  

## 2. Backpropagation Through Time (BPTT)

Since RNNs have recurrent connections, we need a special version of backpropagation to compute gradients efficiently—this is known as **Backpropagation Through Time (BPTT)**.

### **Unrolling the Computation Graph**
Unlike feedforward networks, which have a clear layer-wise structure, RNNs process sequential data with recurrent connections. To compute gradients, we **unroll** the RNN across time steps. This allows us to treat it as a deep feedforward network where each time step corresponds to a layer.

If the input sequence has $T$ time steps, the RNN is unrolled into $T$ layers, where each hidden state depends on the previous one:

[![Hidden State Equation](https://latex.codecogs.com/png.latex?%5Cmathbf%7Bh%7D_t%20%3D%20%5Cphi%28%5Cmathbf%7BW_h%7D%20%5Cmathbf%7Bh%7D_%7Bt-1%7D%20%2B%20%5Cmathbf%7BW_x%7D%20%5Cmathbf%7Bx%7D_t%20%2B%20%5Cmathbf%7Bb%7D%29)](https://latex.codecogs.com/eqneditor/editor.php)

[![Output Computation Equation](https://latex.codecogs.com/png.latex?%5Cmathbf%7Bo%7D_t%20%3D%20%5Cmathbf%7BW_o%7D%20%5Cmathbf%7Bh%7D_t%20%2B%20%5Cmathbf%7Bb_o%7D)](https://latex.codecogs.com/eqneditor/editor.php)

where:
- $\mathbf{h}_t$ is the hidden state at time $t$.
- $\mathbf{x}_t$ is the input at time $t$.
- $\mathbf{o}_t$ is the output at time $t$.
- $\mathbf{W_h}$, $\mathbf{W_x}$, $\mathbf{W_o}$, $\mathbf{b}$, and $\mathbf{b}_o$ are weight matrices and bias terms.

---

### **Loss Computation**
For a given sequence, the loss function aggregates errors across time:

$$
\mathcal{L} = \sum_{t=1}^{T} \ell(\mathbf{o}_t, \mathbf{y}_t)
$$

where:
- $\ell(\mathbf{o}_t, \mathbf{y}_t)$ is the loss at time $t$, comparing the model output $\mathbf{o}_t$ to the target $\mathbf{y}_t$.

---

### **Computing Gradients Using BPTT**
To update weights using gradient descent, we compute **gradients of the loss with respect to each parameter**.

#### **Gradient of the Loss with Respect to Output Weights**
For the output layer:

$$
\frac{\partial \mathcal{L}}{\partial \mathbf{W_o}} = \sum_{t=1}^{T} \frac{\partial \ell}{\partial \mathbf{o}_t} \mathbf{h}_t^\top
$$

Since $\mathbf{o}_t = \mathbf{W_o} \mathbf{h}_t + \mathbf{b}_o$, we get:

$$
\frac{\partial \mathbf{o}_t}{\partial \mathbf{W_o}} = \mathbf{h}_t
$$

so,

$$
\frac{\partial \mathcal{L}}{\partial \mathbf{W_o}} = \sum_{t=1}^{T} \frac{\partial \ell}{\partial \mathbf{o}_t} \mathbf{h}_t^\top.
$$

#### **Gradient of the Loss with Respect to Hidden States**
Since hidden states depend on previous hidden states:

[![Gradient of Loss with Respect to Hidden States](https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cpartial%20%5Cmathcal%7BL%7D%7D%7B%5Cpartial%20%5Cmathbf%7Bh%7D_t%7D%20%3D%20%5Cfrac%7B%5Cpartial%20%5Cell%7D%7B%5Cpartial%20%5Cmathbf%7Bo%7D_t%7D%20%5Cfrac%7B%5Cpartial%20%5Cmathbf%7Bo%7D_t%7D%7B%5Cpartial%20%5Cmathbf%7Bh%7D_t%7D%20%2B%20%5Cfrac%7B%5Cpartial%20%5Cmathcal%7BL%7D%7D%7B%5Cpartial%20%5Cmathbf%7Bh%7D_%7Bt%2B1%7D%7D%20%5Cfrac%7B%5Cpartial%20%5Cmathbf%7Bh%7D_%7Bt%2B1%7D%7D%7B%5Cpartial%20%5Cmathbf%7Bh%7D_t%7D)](https://latex.codecogs.com/eqneditor/editor.php)

This recursive dependency makes the gradient computation more complex.

#### **Gradient of the Loss with Respect to Recurrent Weights**
For $\mathbf{W_h}$, we use the chain rule:

$$
\frac{\partial \mathcal{L}}{\partial \mathbf{W_h}} = \sum_{t=1}^{T} \frac{\partial \mathcal{L}}{\partial \mathbf{h}_t} \frac{\partial \mathbf{h}_t}{\partial \mathbf{W_h}}
$$

Since 

[![Hidden State Equation Again](https://latex.codecogs.com/png.latex?%5Cmathbf%7Bh%7D_t%20%3D%20%5Cphi%28%5Cmathbf%7BW_h%7D%20%5Cmathbf%7Bh%7D_%7Bt-1%7D%20%2B%20%5Cmathbf%7BW_x%7D%20%5Cmathbf%7Bx%7D_t%20%2B%20%5Cmathbf%7Bb%7D%29)](https://latex.codecogs.com/eqneditor/editor.php)

we get:

[![Gradient of Loss with Respect to Recurrent Weights](https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cpartial%20%5Cmathbf%7Bh%7D_t%7D%7B%5Cpartial%20%5Cmathbf%7BW_h%7D%7D%20%3D%20%5Cphi%27%28%5Cmathbf%7BW_h%7D%20%5Cmathbf%7Bh%7D_%7Bt-1%7D%20%2B%20%5Cmathbf%7BW_x%7D%20%5Cmathbf%7Bx%7D_t%20%2B%20%5Cmathbf%7Bb%7D%29%20%5C%2C%5Cmathbf%7Bh%7D_%7Bt-1%7D%5E%5Ctop)](https://latex.codecogs.com/eqneditor/editor.php)

which propagates errors backward through time.

---

## **3. Vanishing and Exploding Gradients**
Since the recurrence involves multiplying gradients through time steps, repeated multiplication by small or large values leads to:
- **Vanishing gradients**: Gradients shrink, making learning difficult.
- **Exploding gradients**: Gradients grow too large, causing instability.

**Solution**: Advanced architectures (LSTMs, GRUs) handle these issues.

---

## Solution to finding different components in  the [Simple RNN Implementation](https://colab.research.google.com/drive/1CXyozxh25-LYTZQ-gXP2bd0LuYLmFdte?usp=sharing)

### What to Look For - Input Layer (x_t)
Look For: The part where the input is defined, usually as one-hot encoded vectors or an embedding layer.

```python
def one_hot_encoding(char_idx, vocab_size):
    x = np.zeros(vocab_size)
    x[char_idx] = 1.0
    return x

def to_one_hot(data, vocab_size):
    return np.array([one_hot_encoding(char_to_idx[ch], vocab_size) for ch in data])
```

Explanation: The one_hot_encoding function defines how a character is represented as a one-hot vector. The to_one_hot function applies this encoding to each character in the input sequence.

### What to Look For - Hidden Layer (h_t)
Look For: The code where the hidden state is updated using the input and the previous hidden state.

```python
def rnn(inputs, state, params):
    W_xh, W_hh, b_h, W_hq, b_q = params
    H = state
    outputs = []

    for X in inputs:
        H = np.tanh(np.dot(X, W_xh) + np.dot(H, W_hh) + b_h)  # Hidden state update
        Y = np.dot(H, W_hq) + b_q
        outputs.append(Y)

    return outputs, H
```

Explanation: The hidden state `H` is updated at each time step using the current input `X` and the previous hidden state `H`. The `tanh` activation function introduces non-linearity. This updated hidden state is carried forward through the sequence, preserving temporal dependencies.

### What to Look For - Output Layer (y_t)
Look For: The part where the output is generated at each time step.

```python
Y = np.dot(H, W_hq) + b_q
```

Explanation: The output Y is generated by applying a linear transformation to the hidden state H, followed by adding the bias b_q.

### What to Look For - Parameter Matrices (Weights and Biases)
Look For: The matrices that represent the weights and biases for each layer in the RNN.

```python
def init_rnn_params(num_inputs, num_hiddens, num_outputs):
    def normal(shape):
        return np.random.randn(*shape) * 0.01

    W_xh = normal((num_inputs, num_hiddens))
    W_hh = normal((num_hiddens, num_hiddens))
    b_h = np.zeros(num_hiddens)
    W_hq = normal((num_hiddens, num_outputs))
    b_q = np.zeros(num_outputs)

    return W_xh, W_hh, b_h, W_hq, b_q
```

Explanation: The function init_rnn_params initializes the weight matrices (W_xh, W_hh, W_hq) and biases (b_h, b_q). These represent the parameters of the RNN that will be updated during training.

### What to Look For - Recurrent Connection (Hidden State Update)
Look For: The loop that updates the hidden state at each time step, which carries information from one step to the next.

```python
for X in inputs:
    H = np.tanh(np.dot(X, W_xh) + np.dot(H, W_hh) + b_h)  # Hidden state update
```

Explanation: At each time step, the hidden state H is updated using the previous hidden state (H) and the current input (X). This recurrent connection carries information through the sequence.

### What to Look For - Loss Calculation
Look For: The section of the code where the loss is calculated by comparing the predicted output with the true output, often using a loss function like cross-entropy.

```python
def cross_entropy_loss(pred, label):
    return -np.sum(label * np.log(pred + 1e-8))

loss = sum(cross_entropy_loss(softmax(o), y) for o, y in zip(outputs, Y))
```

Explanation: The loss is calculated using the cross-entropy function between the predicted output (after applying softmax) and the true label. The total loss is the sum of losses across all time steps. This loss is used to compute gradients, which update the model parameters.

### What to Look For - Training Loop
Look For: The iterative process that adjusts the parameters (weights and biases) based on the gradients and loss.

```python
for epoch in range(num_epochs):
    state = np.zeros(num_hiddens)
    total_loss = 0
    for X, Y in data_iterator(corpus, seq_length, batch_size):
        outputs, state = rnn(X, state, params)
        loss = sum(cross_entropy_loss(softmax(o), y) for o, y in zip(outputs, Y))
        total_loss += loss

        # Gradients calculation and parameter update
        ...
```

Explanation: At the beginning of each epoch, the hidden state is initialized to zeros. During training, the state is updated after processing each batch, ensuring temporal continuity. Gradients are computed and used to update the parameters after each batch.

---

### What to Look For - Text Generation
Look For: The section of the code where the model generates text based on an initial prefix.

```python
def generate_text(prefix, num_chars):
    state = np.zeros(num_hiddens)
    output = list(prefix)

    for ch in prefix:
        X = one_hot_encoding(char_to_idx[ch], vocab_size)
        _, state = rnn([X], state, params)

    for _ in range(num_chars):
        X = one_hot_encoding(char_to_idx[output[-1]], vocab_size)
        Y, state = rnn([X], state, params)
        next_char = idx_to_char[np.argmax(Y[0])]
        output.append(next_char)

    return ''.join(output)
```

Explanation: The text generation function starts with an initial prefix, then iteratively generates subsequent characters by predicting one character at a time, using the output from the RNN as input for the next prediction. The hidden state is updated with each new character, ensuring the model retains context from prior characters.
