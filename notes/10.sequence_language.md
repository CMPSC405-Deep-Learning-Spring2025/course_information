# Sequence Models and Language Models

## Reference: [Colab Notebook](https://colab.research.google.com/drive/1kagKKCIp0IifZw1UCaMcWF0j8s3waVP5?usp=sharing)

## 1. Overview
Sequential data, including text, speech, and even DNA, requires models that capture context over time or position. Natural Language Processing (NLP) is a key example where sequence models track linguistic patterns and dependencies within text. Below are some common NLP tasks illustrating the need for sequence-aware approaches:

- Sentiment Analysis: Determining whether a piece of text is positive, negative, or neutral.
  - Example: Analyzing customer reviews of a product to understand overall satisfaction.

- Text Classification: Assigning predefined labels to text.
  - Example: Categorizing emails as "spam" or "not spam."

- Named Entity Recognition (NER): Identifying and classifying proper names in text.
  - Example: Recognizing "New York" as a location and "Elon Musk" as a person in a news article.

- Machine Translation: Translating text from one language to another.
  - Example: Translating a sentence from English to French.

- Speech Recognition: Converting spoken language into written text.
  - Example: Dictating a message to a virtual assistant like Siri or Alexa.

- Text Summarization: Creating a short summary of a longer piece of text.
  - Example: Summarizing an article or research paper into a few sentences.

- Question Answering: Extracting answers to questions from a body of text.
  - Example: Answering "Who won the 2020 U.S. presidential election?" using news articles.

- Part-of-Speech Tagging: Identifying the grammatical parts of a sentence (e.g., nouns, verbs, adjectives).
  - Example: "She quickly ran to the store" → "She (pronoun), quickly (adverb), ran (verb), to (preposition), the (article), store (noun)."

- Coreference Resolution: Determining which words or phrases refer to the same entity.
  - Example: In the sentence "John went to the store. He bought milk." – Resolving that "He" refers to "John."

- Language Generation: Creating coherent and contextually appropriate text.
  - Example: GPT-based models generating conversational responses based on user input.

## 2. Understanding Sequential Data
- **Sequence**: An ordered list of elements (e.g., words in a sentence).
- **Temporal Dependencies**: Earlier elements affect later ones (e.g., meaning of a sentence depends on word order).
- **Examples**:
  - Sentences in natural language processing (NLP)
  - Stock price predictions
  - DNA sequences in bioinformatics

Text must be converted into numbers before models can use it.
- Step 1: Tokenization
Tokenization splits text into words or characters.
[Regular Expression Tester](https://regexr.com/)

- Step 2: Conversion of tokens to numbers
### Word Embeddings and One-Hot Encoding
- **One-hot encoding**: Represent words as vectors with a single 1 (e.g., "cat" = [0,1,0,...,0]).
- **Word embeddings**: Learn dense vector representations (e.g., Word2Vec, GloVe) where similar words have similar embeddings.

## 3. Perplexity and Sequence Partitioning

### 3.1 Perplexity
$$
\text{Perplexity} = \exp\!\Bigl( - \frac{1}{N} \sum_{i=1}^{N} \log P(w_i \mid w_1, \ldots, w_{i-1}) \Bigr)
$$
- **Interpretation**:
  - Lower perplexity = better model.
  - Perplexity of a random model is high, while a perfect model has perplexity close to 1.

### 3.2 Partitioning Sequences
- **Goal**: Prepare text for training by dividing it into manageable chunks.
- **Common Methods**:
  - **Fixed-length partitioning**: Split text into fixed-size sequences.
  - **Overlapping windows**: Ensure continuity across partitions.
  - **Batch processing**: Improves training efficiency.

## 4. N-Gram Models
- **Definition**: An n-gram is a sequence of **n** consecutive words.
- **Example (bigram model, n=2)**:
  - "The cat sits" → P("sits" | "The cat")
- **Limitations**:
  - Fixed window size (cannot capture long-term dependencies).
  - Data sparsity (rare sequences are hard to model).

## 5. Language Modeling
- **Goal**: Predict the next word given a sequence.
- **Markov Assumption**: Probability of a word depends only on the previous **n-1** words.
- **Mathematical Formulation**:  
  Given sequence w1, w2, ..., wT, model:
  ```
  P(w_T | w_1, w_2, ..., w_{T-1})
  ```
- **Challenges**:
  - Longer contexts require more data.
  - Computational cost increases with larger n.

### 5.1 Types of Language Models
A language model (LM) assigns probabilities to sequences of words:
```
P(w_T | w_1, w_2, ..., w_{T-1})
```

**Unigram Model (Ignores Order)**
```
P(w_T) = count(w_T) / total_words
```
Problem: It doesn't consider context.

**Bigram Model (Considers One Previous Word)**
```
P(w_T | w_{T-1}) = count(w_{T-1}, w_T) / count(w_{T-1})
```

**N-gram Model (Considers N−1 Previous Words)**
```
P(w_T | w_{T-n+1}, ..., w_{T-1})
```

# Recurrent Neural Networks (RNNs) and Backpropagation Through Time (BPTT)

## 1. Introduction to RNNs
- **Why RNNs?**
  - Traditional models (n-grams, bag-of-words) fail to capture long-term dependencies.
  - RNNs maintain a **hidden state** to store past information.
- **Mathematical Representation**:
  ```
  h_t = f(W_h h_{t-1} + W_x x_t + b)
  ```
  where:
  - $W_h$, $W_x$ = weight matrices
  - $h_t$ = hidden state
  - $x_t$ = input at time $t$

## 2. Vanishing & Exploding Gradients
- **Vanishing gradients**: Gradients shrink, making learning difficult.
- **Exploding gradients**: Gradients grow too large, causing instability.
- **Solution**: Advanced architectures (LSTMs, GRUs) handle these issues.

## 3. Backpropagation Through Time (BPTT)
- **Steps**:
  1. Forward propagate through all time steps.
  2. Compute loss at final step.
  3. Backpropagate errors through time.
  4. Update weights at each step.

## 4. Experiment: RNN Text Generation
- **Goal**: Modify an RNN-based text generator and observe effects of sequence length.
- **Questions to explore**:
  - How does sequence length impact learning stability?
  - Does increasing hidden size improve results?
  - How does final loss compare across different lengths?

