# Sequence Models and Language Models

## Reference: [Colab Notebook](https://colab.research.google.com/drive/1kagKKCIp0IifZw1UCaMcWF0j8s3waVP5?usp=sharing)

## 1. Overview
Sequential data, including text, speech, and even DNA, requires models that capture context over time or position. Natural Language Processing (NLP) is a key example where sequence models track linguistic patterns and dependencies within text. Below are some common NLP tasks illustrating the need for sequence-aware approaches:

- Sentiment Analysis: Determining whether a piece of text is positive, negative, or neutral.
  - Example: Analyzing customer reviews of a product to understand overall satisfaction.

- Text Classification: Assigning predefined labels to text.
  - Example: Categorizing emails as "spam" or "not spam."

- Named Entity Recognition (NER): Identifying and classifying proper names in text.
  - Example: Recognizing "New York" as a location and "Elon Musk" as a person in a news article.

- Machine Translation: Translating text from one language to another.
  - Example: Translating a sentence from English to French.

- Speech Recognition: Converting spoken language into written text.
  - Example: Dictating a message to a virtual assistant like Siri or Alexa.

- Text Summarization: Creating a short summary of a longer piece of text.
  - Example: Summarizing an article or research paper into a few sentences.

- Question Answering: Extracting answers to questions from a body of text.
  - Example: Answering "Who won the 2020 U.S. presidential election?" using news articles.

- Part-of-Speech Tagging: Identifying the grammatical parts of a sentence (e.g., nouns, verbs, adjectives).
  - Example: "She quickly ran to the store" → "She (pronoun), quickly (adverb), ran (verb), to (preposition), the (article), store (noun)."

- Coreference Resolution: Determining which words or phrases refer to the same entity.
  - Example: In the sentence "John went to the store. He bought milk." – Resolving that "He" refers to "John."

- Language Generation: Creating coherent and contextually appropriate text.
  - Example: GPT-based models generating conversational responses based on user input.

## 2. Understanding Sequential Data
- **Sequence**: An ordered list of elements (e.g., words in a sentence).
- **Temporal Dependencies**: Earlier elements affect later ones (e.g., meaning of a sentence depends on word order).
- **Examples**:
  - Sentences in natural language processing (NLP)
  - Stock price predictions
  - DNA sequences in bioinformatics

Text must be converted into numbers before models can use it.
- Step 1: Tokenization
Tokenization splits text into words or characters.
[Regular Expression Tester](https://regexr.com/)

- Step 2: Conversion of tokens to numbers
### Word Embeddings and One-Hot Encoding
- **One-hot encoding**: Represent words as vectors with a single 1 (e.g., "cat" = [0,1,0,...,0]).
- **Word embeddings**: Learn dense vector representations (e.g., Word2Vec, GloVe) where similar words have similar embeddings.

## 3. Perplexity and Sequence Partitioning

### 3.1 Perplexity
$$
\text{Perplexity} = \exp\!\Bigl(- \frac{1}{N} \sum_{i=1}^{N} \log P(w_i \mid w_1, \ldots, w_{i-1}) \Bigr)
$$
- **Interpretation**:
  - Lower perplexity = better model.
  - Perplexity of a random model is high, while a perfect model has perplexity close to 1.

### 3.2 Partitioning Sequences
- **Goal**: Prepare text for training by dividing it into manageable chunks.
- **Common Methods**:
  - **Fixed-length partitioning**: Split text into fixed-size sequences.
  - **Overlapping windows**: Ensure continuity across partitions.
  - **Batch processing**: Improves training efficiency.

## 4. N-Gram Models
- **Definition**: An n-gram is a sequence of **n** consecutive words.
- **Example (bigram model, n=2)**:
  - "The cat sits" → P("sits" | "The cat")
- **Limitations**:
  - Fixed window size (cannot capture long-term dependencies).
  - Data sparsity (rare sequences are hard to model).

## 5. Language Modeling
- **Goal**: Predict the next word given a sequence.
- **Markov Assumption**: Probability of a word depends only on the previous **n-1** words.
- **Mathematical Formulation**:  
  Given sequence w1, w2, ..., wT, model: $P(w_T \mid w_1, w_2, \ldots, w_{T-1})$
- **Challenges**:
  - Longer contexts require more data.
  - Computational cost increases with larger n.

### 5.1 Types of Language Models
A language model (LM) assigns probabilities to sequences of words:
$$P(w_{T} \mid w_{1}, w_{2}, \ldots, w_{{T-1}})$$

**Unigram Model (Ignores Order)**
$$P(w_{T}) = \frac{\mathrm{count}(w_{T})}{\mathrm{totalWords}}$$

Problem: It doesn't consider context.

**Bigram Model (Considers One Previous Word)**
$$P(w_{T} \mid w_{{T-1}}) = \frac{\mathrm{count}(w_{{T-1}}, w_{T})}{\mathrm{count}(w_{{T-1}})}$$

**N-gram Model (Considers N−1 Previous Words)**
$$P(w_{T} \mid w_{{T-n+1}}, \ldots, w_{{T-1}})$$

# Recurrent Neural Networks (RNNs) and Backpropagation Through Time (BPTT)

## 1. Introduction to RNNs
Recurrent Neural Networks (RNNs) process sequences step-by-step while maintaining a **hidden state** that captures information from previous inputs. This hidden state acts as memory, enabling the model to learn sequential dependencies. 

- **Why RNNs?**
  - Traditional models (n-grams, bag-of-words) fail to capture long-term dependencies.
  - RNNs maintain a **hidden state** to store past information.

## **Hidden States in RNNs**  

#### **Hidden State Update Equation**  
At each time step $t$, the hidden state $\mathbf{h}_t$ is updated using:

$$ 
\mathbf{h}_t = \phi(\mathbf{W_h} \mathbf{h}_{t-1} + \mathbf{W_x} \mathbf{x}_t + \mathbf{b}) 
$$

where:  
- $\mathbf{h}_t$ = hidden state at time step $t$  
- $\mathbf{h}_{t-1}$ = hidden state from the previous time step  
- $\mathbf{x}_t$ = input at time step $t$  
- $\mathbf{W_h}$ = weight matrix for the hidden state  
- $\mathbf{W_x}$ = weight matrix for the input  
- $\mathbf{b}$ = bias vector  
- $\phi$ = activation function (typically **tanh** or **ReLU**)  

This equation ensures that past information stored in $\mathbf{h}_{t-1}$ is combined with new input $\mathbf{x}_t$, allowing the network to maintain a memory of past inputs.  

### **Output Computation**  
The hidden state is then used to compute the output $\mathbf{o}_t$ at each time step:  

$$
\mathbf{o}_t = \mathbf{W_o} \mathbf{h}_t + \mathbf{b}_o
$$  

where:  
- $\mathbf{W_o}$ = weight matrix for mapping the hidden state to the output  
- $\mathbf{b}_o$ = bias term for the output  
- $\mathbf{o}_t$ = output at time $t$  

### **Key Takeaways**  
- The recurrence in $\mathbf{h}_t$ allows the model to propagate information over time.  
- The activation function $\phi$ (e.g., tanh) introduces non-linearity, preventing simple linear dependence on previous states.  
- The weight matrices $\mathbf{W_h}$ and $\mathbf{W_x}$ are **learned during training**, allowing the model to extract meaningful sequential patterns.  

## 2. Backpropagation Through Time (BPTT)

Since RNNs have recurrent connections, we need a special version of backpropagation to compute gradients efficiently—this is known as **Backpropagation Through Time (BPTT)**.

### **Unrolling the Computation Graph**
Unlike feedforward networks, which have a clear layer-wise structure, RNNs process sequential data with recurrent connections. To compute gradients, we **unroll** the RNN across time steps. This allows us to treat it as a deep feedforward network where each time step corresponds to a layer.

If the input sequence has $T$ time steps, the RNN is unrolled into $T$ layers, where each hidden state depends on the previous one:

$$
\mathbf{h}_t = \phi(\mathbf{W_h} \mathbf{h}_{t-1} + \mathbf{W_x} \mathbf{x}_t + \mathbf{b})
$$

$$
\mathbf{o}_t = \mathbf{W_o} \mathbf{h}_t + \mathbf{b}_o
$$

where:
- $\mathbf{h}_t$ is the hidden state at time $t$.
- $\mathbf{x}_t$ is the input at time $t$.
- $\mathbf{o}_t$ is the output at time $t$.
- $\mathbf{W_h}$, $\mathbf{W_x}$, $\mathbf{W_o}$, $\mathbf{b}$, and $\mathbf{b}_o$ are weight matrices and bias terms.

---

### **Loss Computation**
For a given sequence, the loss function aggregates errors across time:

$$
\mathcal{L} = \sum_{t=1}^{T} \ell(\mathbf{o}_t, \mathbf{y}_t)
$$

where:
- $\ell(\mathbf{o}_t, \mathbf{y}_t)$ is the loss at time $t$, comparing the model output $\mathbf{o}_t$ to the target $\mathbf{y}_t$.

---

### **Computing Gradients Using BPTT**
To update weights using gradient descent, we compute **gradients of the loss with respect to each parameter**.

#### **Gradient of the Loss with Respect to Output Weights**
For the output layer:

$$
\frac{\partial \mathcal{L}}{\partial \mathbf{W_o}} = \sum_{t=1}^{T} \frac{\partial \ell}{\partial \mathbf{o}_t} \mathbf{h}_t^\top
$$

Since $\mathbf{o}_t = \mathbf{W_o} \mathbf{h}_t + \mathbf{b}_o$, we get:

$$
\frac{\partial \mathbf{o}_t}{\partial \mathbf{W_o}} = \mathbf{h}_t
$$

so,

$$
\frac{\partial \mathcal{L}}{\partial \mathbf{W_o}} = \sum_{t=1}^{T} \frac{\partial \ell}{\partial \mathbf{o}_t} \mathbf{h}_t^\top.
$$

#### **Gradient of the Loss with Respect to Hidden States**
Since hidden states depend on previous hidden states:

$$
\frac{\partial \mathcal{L}}{\partial \mathbf{h}_t} = \frac{\partial \ell}{\partial \mathbf{o}_t} \frac{\partial \mathbf{o}_t}{\partial \mathbf{h}_t} + \frac{\partial \mathcal{L}}{\partial \mathbf{h}_{t+1}} \frac{\partial \mathbf{h}_{t+1}}{\partial \mathbf{h}_t}
$$

This recursive dependency makes the gradient computation more complex.

#### **Gradient of the Loss with Respect to Recurrent Weights**
For $\mathbf{W_h}$, we use the chain rule:

$$
\frac{\partial \mathcal{L}}{\partial \mathbf{W_h}} = \sum_{t=1}^{T} \frac{\partial \mathcal{L}}{\partial \mathbf{h}_t} \frac{\partial \mathbf{h}_t}{\partial \mathbf{W_h}}
$$

Since 

$$
\mathbf{h}_t = \phi(\mathbf{W_h} \mathbf{h}_{t-1} + \mathbf{W_x} \mathbf{x}_t + \mathbf{b})
$$

we get:

$$
\frac{\partial \mathbf{h}_t}{\partial \mathbf{W_h}} = \phi'(\mathbf{W_h} \mathbf{h}_{t-1} + \mathbf{W_x} \mathbf{x}_t + \mathbf{b}) \,\mathbf{h}_{t-1}^\top
$$

which propagates errors backward through time.

---

## **3. Vanishing and Exploding Gradients**
Since the recurrence involves multiplying gradients through time steps, repeated multiplication by small or large values leads to:
- **Vanishing gradients**: Gradients shrink, making learning difficult.
- **Exploding gradients**: Gradients grow too large, causing instability.

**Solution**: Advanced architectures (LSTMs, GRUs) handle these issues.


