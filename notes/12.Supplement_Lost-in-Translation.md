## "Lost in Translation" Activity

In this activity, you will work in groups to simulate how a Seq2Seq model translates text. You will experience the challenges of encoding, decoding, and improving translations using an attention-like mechanism.

## Instructions

### Step 1: Assign Roles
Each group will have four roles:

- **Encoder**: Reads the full sentence and summarizes it using 3-5 keywords (context vector).

- **Decoder**: Uses only the keywords to reconstruct the sentence in either English or a target language (e.g., Spanish, French).

- **Evaluator**: Compares the original sentence with the decoded sentence and scores it on accuracy (1-10 scale) - can use Google translate for accuracy.

- **Observer**: Takes notes on what information was lost or misunderstood.

### Step 2: Translate Without Context
- The Encoder will write down 3-5 keywords summarizing the original sentence (3 for shorter sentence, 5-7 for longer ones).

- The Decoder will translate the sentence using the dictionary based on only these keywords. 

- The Evaluator will compare the original and decoded sentences, noting differences.

### Step 3: Challenges
Now, try translating longer sentences or ambiguous phrases.

Take note of where meaning is lost or where key details are missing.

### Step 4: Improve with "Attention"
This time, the Decoder may ask the Encoder for 1-2 additional keywords to help clarify the sentence.

Compare how the new sentence improves over the first attempt.

### Step 5: Group Discussion
Discuss the following questions as a group:

- What made some translations harder than others?

- How did adding more context (attention) improve the translation?

- How do real translation models deal with long or complex sentences?