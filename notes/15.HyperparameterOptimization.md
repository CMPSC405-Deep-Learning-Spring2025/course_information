# Hyperparameter Optimization?

- *Ref: [19.1 Hyperparameter Optimization?](https://d2l.ai/chapter_hyperparameter-optimization)*

## What Are Hyperparameters?

- **Definition**: Hyperparameters are parameters set before training a model that influence its learning process and performance.

- **Examples**:
  - **Learning Rate**: Controls the step size during gradient descent.
  - **Batch Size**: Number of training samples used in one iteration.
  - **Number of Layers/Units**: Determines the depth and width of the model.
  - **Regularization Parameters**: Such as dropout rate or weight decay, to prevent overfitting.
  
## The Optimization Problem

- **Objective**: Find a set of hyperparameters that minimize the validation error of the model.

- **Challenges**:
  - **Nested Optimization**: Each evaluation of a hyperparameter configuration requires training and validating a model, making the process computationally expensive.
  - **Noisy Evaluations**: Validation errors can be noisy, leading to unreliable assessments of hyperparameter configurations.
  - **High Dimensionality**: The search space for hyperparameters can be vast, especially with complex models.

## Random Search

- **Method**: Randomly sample hyperparameter configurations from the search space.

- **Advantages**:
  - **Simplicity**: Easy to implement and understand.
  - **Effectiveness**: Can outperform grid search, especially when only a few hyperparameters significantly affect performance.

- **Considerations**:
  - **Efficiency**: May require a large number of evaluations to find optimal configurations.
  - **Exploration vs. Exploitation**: Balancing the exploration of new configurations with the exploitation of known good ones.

## Other Things to Consider

1. **Validation Set Usage**:
   - Using the original training set for training and the test set for validation can lead to overfitting.
   - **Solution**: Implement proper cross-validation techniques to ensure the model generalizes well.

2. **Gradient-Based Optimization**:
   - Using gradient descent for hyperparameter optimization is challenging due to issues like vanishing/exploding gradients and computational complexity.
   - **Alternative**: Consider using methods like random search or Bayesian optimization.

3. **Grid Search vs. Random Search**:
   - Grid search systematically explores the hyperparameter space, while random search samples configurations randomly.
   - **Efficiency**: Random search can be more efficient, especially when only a few hyperparameters significantly impact performance.

---

# Hyperparameter Decision Challenge:

Optimize hyperparameters for three popular deep learning architectures: Recurrent Neural Networks (RNN), Convolutional Neural Networks (CNN), and Transformer models. Your goal is to select the best hyperparameters based on the task at hand.

### Instructions:

1. **Form Teams**:  
   Divide into small teams (three total).

2. **Receive Your Scenario**:  
   Each team will be assigned a scenario below describing a machine learning task related to either an RNN, CNN, or Transformer model. Read the task carefully, and decide what hyperparameters would optimize the model for the given task.

3. **Discuss and Decide**:  
   Each team will have **10 minutes** to discuss and select the best hyperparameter settings for the model based on the scenario provided. Write down your choices on the whiteboard or notepad.

4. **Present Your Decisions**:  
   After 10 minutes, each team will present their choices to the class and explain the rationale behind their decisions. Focus on how each hyperparameter setting addresses the specific task's challenges.

5. **Feedback and Reflection**:  
   After each team presents, I will provide feedback on the choices and discuss whether other configurations might be better. 

---

### Scenarios:

#### **Scenario 1: Text Generation with RNN**

**Context**: You are tasked with building a character-level language model using a Recurrent Neural Network (RNN) to generate coherent text. The dataset consists of a large corpus of literary texts (e.g., Shakespeareâ€™s works). You need to adjust hyperparameters to generate realistic and coherent sentences.

**Your Task**:  
- Choose the best hyperparameters to train the RNN model for text generation.

**Hyperparameters to Consider**:
- Learning rate (e.g., 0.001, 0.01, 0.1)
- Number of layers (e.g., 1, 2, 3)
- Number of units in each layer (e.g., 64, 128, 256)
- Dropout rate (e.g., 0.1, 0.2, 0.5)
- Batch size (e.g., 32, 64, 128)

**Guiding Questions**:
- What learning rate will help the model converge without overshooting or stalling?
- How many layers and units should the RNN have to generate coherent text without overfitting?
- Will a dropout rate be needed to prevent overfitting with a large corpus?

---

#### **Scenario 2: Image Classification with CNN**

**Context**: You need to build a Convolutional Neural Network (CNN) to classify images from the CIFAR-10 dataset, which contains 60,000 32x32 color images in 10 classes. You need to optimize the CNN architecture for accuracy and training efficiency.

**Your Task**:  
- Select the optimal hyperparameters for the CNN architecture to perform well on the CIFAR-10 dataset.

**Hyperparameters to Consider**:
- Learning rate (e.g., 0.001, 0.01, 0.1)
- Number of layers (e.g., 3, 5, 8)
- Number of filters in each convolutional layer (e.g., 32, 64, 128)
- Kernel size (e.g., 3x3, 5x5, 7x7)
- Batch size (e.g., 32, 64, 128)
- Dropout rate (e.g., 0.2, 0.5)

**Guiding Questions**:
- What number of filters and kernel size will allow the model to capture important features in the image?
- How many layers should you use for a good balance between depth and training time?
- What batch size will allow for efficient training without consuming too much memory?

---

#### **Scenario 3: Machine Translation with Transformer**

**Context**: You need to build a machine translation model using the Transformer architecture to translate English sentences into French. The dataset contains parallel English-French sentence pairs. You need to optimize the Transformer model for translation quality and efficiency.

**Your Task**:  
- Select the best hyperparameters for the Transformer architecture to achieve high-quality translations.

**Hyperparameters to Consider**:
- Learning rate (e.g., 0.001, 0.0005)
- Number of layers (e.g., 4, 6, 8)
- Number of attention heads (e.g., 8, 16, 32)
- Hidden size (e.g., 256, 512, 1024)
- Dropout rate (e.g., 0.1, 0.3)
- Batch size (e.g., 64, 128)

**Guiding Questions**:
- What number of attention heads should you use to effectively capture relationships between words in the sentence?
- How many layers should the model have to balance training time and translation quality?
- What hidden size will give enough capacity to the model for complex sentence translation?
