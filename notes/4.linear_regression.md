# Linear Regression

Linear regression is a machine learning algorithm used for predicting a *continuous* target variable based on one or more input features. It extends the concept of the linear equation to multiple dimensions and assumes a linear relationship between the input features and the target variable.

## Linear Equation

A simple linear equation can be written as `y = mx + b`, where:

- `y` is the dependent variable (output).
- `x` is the independent variable (input).
- `m` is the slope of the line, which determines the inclination of the line.
- `b` is the intercept, which represents the bias term that shifts the line up or down.

This equation describes a straight line in a two-dimensional space, where `y` changes linearly with `x`.

## Model

The linear regression model can be represented as:

![Linear Regression Model](https://latex.codecogs.com/png.latex?\hat{y}=\mathbf{w}^T\mathbf{x}+b)

where:
- ![Predicted Output](https://latex.codecogs.com/png.latex?\hat{y}) is the predicted output.
- ![Weight Vector](https://latex.codecogs.com/png.latex?\mathbf{w}) is the weight vector.
- ![Input Feature Vector](https://latex.codecogs.com/png.latex?\mathbf{x}) is the input feature vector.
- ![Bias Term](https://latex.codecogs.com/png.latex?b) is the bias term.

The bias term `b` allows the model to fit the data better by shifting the regression line up or down. It accounts for the baseline level of the target variable when all input features are zero.

### Assumptions of Linear Regression

1. Linearity of the relationship between inputs and output.
2. Independence of the errors.
3. Homoscedasticity (constant variance of errors).
4. Normality of error distribution (often assumed for small sample sizes).

Below is an image demonstrating linear regression, where the red line represents the best fit line through the data points:

![Linear Regression Graph](https://upload.wikimedia.org/wikipedia/commons/3/3a/Linear_regression.svg)

In this graph:
- The **blue dots** represent the actual data points.
- The **red line** is the regression line, which is the best fit line that minimizes the sum of the squared differences between the actual data points and the predicted values on the line. This line represents the linear relationship between the input feature and the target variable.

### Least-Squares Error

We can use a *least squares* method to discover the best-fit line for a set of paired data. 

The goal is to find parameters (`m` and `b`) that minimize:
![Least Squares Formula](https://latex.codecogs.com/png.latex?\sum_{i=1}^{n}(y_i-\hat{y}_i)^2)

By minimizing the sum of squared differences we not only measure how far each predicted point is from the actual point, but also penalize larger errors more heavily than smaller ones. Solving for the parameters (e.g., slope `m` and intercept `b`) involves taking partial derivatives of the sum with respect to each parameter and setting them to zero. This yields a set of equations known as the "normal equations," which can be solved directly to find the optimal line. This process ensures that the total squared error between the observed data points and the line is as small as possible.

By taking partial derivatives of this sum with respect to `m` and `b` and setting them to zero, we solve for the optimal slope and intercept, leading to the closed-form least-squares solutions.

## Loss Function

The loss function measures the difference between the predicted output and the actual target value. The most commonly used loss function for linear regression is the **Mean Squared Error (MSE)**:

![MSE](https://latex.codecogs.com/png.latex?\text{MSE}=\frac{1}{n}\sum_{i=1}^{n}(y_i-\hat{y}_i)^2)

where:
- ![Number of Samples](https://latex.codecogs.com/png.latex?n) is the number of samples.
- ![Actual Target Value](https://latex.codecogs.com/png.latex?y_i) is the actual target value.
- ![Predicted Value](https://latex.codecogs.com/png.latex?\hat{y}_i) is the predicted value.

## Gradient Descent

**Gradient Descent** is an optimization algorithm used to minimize the loss function by iteratively updating the model parameters (weights and bias). The update rules for the weights and bias are:

![Weight Update](https://latex.codecogs.com/png.latex?\mathbf{w}\leftarrow\mathbf{w}-\eta\frac{\partial{L}}{\partial{\mathbf{w}}})

![Bias Update](https://latex.codecogs.com/png.latex?b\leftarrow{b}-\eta\frac{\partial{L}}{\partial{b}})

where:
- ![Learning Rate](https://latex.codecogs.com/png.latex?\eta) is the learning rate.
- ![Loss Function](https://latex.codecogs.com/png.latex?L) is the loss function.

## Steps of Gradient Descent

### Step 1: Initialize Parameters

- Initialize the weights \(\mathbf{w}\) and bias \(b\) with small random values or zeros.

### Step 2: Compute Predictions

- Compute the predicted output \(\hat{y}\) using the current weights and bias.

### Step 3: Compute Loss

- Calculate the loss using the loss function (e.g., MSE).

### Step 4: Compute Gradients

- Compute the gradients of the loss with respect to the weights and bias.

### Step 5: Update Parameters

- Update the weights and bias using the gradients and the learning rate.

### Step 6: Repeat

- Repeat steps 2-5 for a specified number of iterations or until convergence.

## Example Code

Here is an example implementation of linear regression using gradient descent in Python:

```python
import numpy as np

class LinearRegression:
    def __init__(self, input_size, lr=0.01):
        self.weights = np.zeros(input_size + 1)
        self.lr = lr

    def predict(self, x):
        return self.weights.T.dot(np.insert(x, 0, 1))

    def fit(self, X, y, epochs=1000):
        for _ in range(epochs):
            for i in range(y.shape[0]):
                x = np.insert(X[i], 0, 1)
                y_pred = self.predict(x)
                error = y[i] - y_pred
                self.weights += self.lr * error * x

# Example usage
if __name__ == "__main__":
    X = np.array([[1], [2], [3], [4]])
    y = np.array([2, 3, 4, 5])
    model = LinearRegression(input_size=1)
    model.fit(X, y)
    print("Predictions:", model.predict(X))
```

## Key Points

- Linear regression assumes a linear relationship between input features and the target variable.
- The model parameters (weights and bias) are optimized to minimize the loss function.
- Gradient descent is a common optimization algorithm used to update the model parameters iteratively.
- The learning rate controls the step size during optimization.
- Proper initialization of parameters and careful tuning of the learning rate are crucial for effective training.

Linear regression is a simple yet powerful algorithm for regression tasks. Understanding its underlying principles and implementation provides a solid foundation for more advanced machine learning algorithms.