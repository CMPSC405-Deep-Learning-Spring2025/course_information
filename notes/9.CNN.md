# **Convolutional Neural Networks (CNNs)**

## **Why Do We Need CNNs?**
### **Limitations of Fully Connected Networks**
- Fully connected networks (FCNs) treat all pixels as independent, ignoring spatial relationships.
- Flattening an image into a 1D vector discards **local structure**.
- A small shift in an image changes the input drastically, causing poor generalization.

### **How CNNs Solve This Problem**
- CNNs use **local receptive fields** to capture meaningful structures.
- CNNs apply **shared filters** across the image, reducing parameters.
- CNNs have **translation invariance**, meaning they recognize patterns anywhere in the image.

---

## **Key Concepts in CNNs**

### **1. Locality in Images**
- Pixels close to each other share meaningful relationships.
- Instead of looking at the entire image at once, CNNs focus on small regions.
- **Example:** A **3×3 filter** extracts information from a small patch in a 5×5 image.
- **Key takeaway:** Local receptive fields capture edges, textures, and shapes.

### **2. Convolution Operation**
- CNNs slide a small **filter (kernel)** over the image to detect features.
- **Mathematical formula:**  

$Y[h,w] = \sum_{i=-k}^k \sum_{j=-k}^k X[h+i,w+j] \cdot K[i,j]$

Where:
- $X$ is the input image
- $K$ is the convolution kernel (filter)
- $Y[h,w]$ is the output feature map at position (h,w)
- $h,w$ are the height and width coordinates in the output feature map
- $i,j$ are the kernel coordinates, ranging from -k to k (for a (2k+1)×(2k+1) kernel)

- **Parameter Sharing:** The same set of weights (values in the filter) is applied across all locations of the input image. This drastically reduces the total number of parameters and allows the network to learn the same feature(s) regardless of position in the image.

### **3. Translation Invariance**
- CNNs can recognize features **regardless of their position**.
- If a network learns to detect an edge in one part of the image, it can detect it anywhere.
- Unlike FCNs, CNNs generalize well across shifted inputs.

### **4. Image Channels**
- Images can have multiple channels (e.g., RGB images have 3 channels: Red, Green, Blue).
- Convolution filters also have depth to process multiple channels.
- **Note**: Each convolutional layer outputs multiple feature maps, collectively acting as the layer's "channels". These channels from one layer become the input channels for the next layer, capturing increasingly complex features. Hence, in a CNN layer, **channels** refer to the number of distinct filters (kernels) applied to the input image or previous layer's output.

### **5. Feature Maps and Receptive Fields**
- The output of a convolutional layer is called a **feature map**.
- Each neuron in a feature map is connected to a **local region (receptive field)** of the input.

### **6. Padding, Stride, and Dilation**
- **Padding**:
  - Adds extra pixels around the input to preserve spatial dimensions.
  - Prevents images from shrinking too much after convolutions.
- **Stride**:
  - Controls how much the filter moves at each step.
  - Larger strides reduce the output size and computational cost.
- **Dilation**:
  - Expands the receptive field of the kernel by spacing out elements in the filter (e.g., a dilation of 2 inserts 1 "gap" between filter values). This allows CNNs to capture larger context without increasing the number of parameters.

### **7. Multiple Input Channels in CNNs**
- CNNs process images with multiple channels (e.g., RGB has 3 channels: Red, Green, Blue).
- Each channel has its own filter weights, learning different aspects of the image.
- Outputs are combined into feature maps.

### **8. Pooling**
- Pooling reduces image size while keeping key features.
- Types of pooling: Max pooling (keeps strongest signals) vs. Average pooling (smoothens features).

### **9. LeNet: The First Successful CNN**
- Developed by Yann LeCun (1998) for handwritten digit recognition.

Architecture:
- Two convolutional layers (detects features).
- Two pooling layers (reduces size).
- Fully connected layers (final classification).

LeNet layers:
- `Conv1` (6 filters, 5×5 kernel, activation: ReLU): Extracts features from input image.
- `Pooling1` (2×2 avg pooling): Reduces spatial size, retaining key features.
- `Conv2` (16 filters, 5×5 kernel, activation: ReLU): Detects more complex patterns.
- `Pooling2` (2×2 avg pooling): Further size reduction.
- Fully connected layers (120 → 84 → 10 output classes): Final classification.

## Visualizers:

- [ConvNet](https://convnetplayground.fastforwardlabs.com/#/models)
- [Convolution Visualizer](https://ezyang.github.io/convolution-visualizer/)
---

## **Summary**
- CNNs extract meaningful features using **local receptive fields** and **parameter sharing**.  
- **Translation invariance** allows CNNs to recognize objects regardless of their position.  
- CNNs handle **multi-channel images** using 3D filters.  
- **Padding and stride** control spatial dimensions and computational efficiency.

